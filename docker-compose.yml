services:
  keycloak:
    container_name: peruHCE-keycloak
    build:
      context: ./keycloak
      dockerfile: Dockerfile
    restart: "unless-stopped"
    env_file:
      - ./.env
    environment:
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN:-admin}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD:-admin123}
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://keycloak-db:5432/${KC_DB_DATABASE:-keycloak}
      KC_DB_USERNAME: ${KC_DB_USERNAME:-keycloak}
      KC_DB_PASSWORD: ${KC_DB_PASSWORD:-keycloak123}
      KC_HOSTNAME: ${KC_HOSTNAME:-localhost}
      KC_HOSTNAME_STRICT: "false"
      KC_HTTP_ENABLED: "true"
    ports:
      - "${KEYCLOAK_PORT:-8180}:8080"
    volumes:
      - ./keycloak/themes:/opt/keycloak/themes:ro
    depends_on:
      - keycloak-db
    networks:
      - auth-network

  keycloak-db:
    container_name: peruHCE-keycloak-db
    image: postgres:15
    restart: "unless-stopped"
    env_file:
      - ./.env
    environment:
      POSTGRES_DB: ${KC_DB_DATABASE:-keycloak}
      POSTGRES_USER: ${KC_DB_USERNAME:-keycloak}
      POSTGRES_PASSWORD: ${KC_DB_PASSWORD:-keycloak123}
    ports:
      - "5434:5432"
    volumes:
      - keycloak-data:/var/lib/postgresql/data
    networks:
      - auth-network

  portainer:
    container_name: peruHCE-portainer
    image: portainer/portainer-ce:lts
    command: -H unix:///var/run/docker.sock --http-enabled --base-url=/portainer
    restart: "unless-stopped"
    ports:
      - "9443:9443" # HTTPS Portainer Web UI
      - "9000:9000" # HTTP Portainer Web UI
      - "8000:8000" # Necessary for other reason
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - portainer-data:/data
    networks:
      - management-network

  gateway:
    container_name: peruHCE-gateway
    image: openmrs/openmrs-reference-application-3-gateway:${TAG:-qa}
    restart: "unless-stopped"
    depends_on:
      - frontend
      - backend
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - "/var/run/docker.sock:/tmp/docker.sock:ro"
    networks:
      - frontend-network
      - backend-network
      - auth-network

  frontend:
    container_name: peruHCE-frontend
    image: openmrs/openmrs-reference-application-3-frontend:${TAG:-qa}
    restart: "unless-stopped"
    environment:
      SPA_PATH: /openmrs/spa
      API_URL: /openmrs
      SPA_CONFIG_URLS: /openmrs/spa/config-openmrs.json,/openmrs/spa/config-pucp.json #,/openmrs/spa/oauth2.json
      SPA_DEFAULT_LOCALE: es
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost/" ]
      timeout: 5s
    depends_on:
      - backend
    volumes:
      - openmrs-frontend:/usr/share/nginx/html/
    networks:
      - frontend-network
      - backend-network
      - auth-network

  backend:
    container_name: peruHCE-backend
    image: openmrs/openmrs-reference-application-3-backend:${TAG:-qa}
    restart: "unless-stopped"
    ports:
      - "8080:8080"
    depends_on:
      - db-replic
      - keycloak
    #env_file: "enviromentVariables.env"
    environment:
      ENABLE_SSO: "true" # Enable Single Sign-On (colocar como variable de entorno? si lo queremos hacer flexible tambien habria que hacerlo en el frontend)
      OMRS_CONFIG_MODULE_WEB_ADMIN: "true"
      OMRS_CONFIG_AUTO_UPDATE_DATABASE: "true"
      OMRS_CONFIG_CREATE_TABLES: "true"
      OMRS_CONFIG_CONNECTION_SERVER: db
      OMRS_CONFIG_CONNECTION_DATABASE: openmrs
      OMRS_CONFIG_CONNECTION_USERNAME: ${OPENMRS_DB_USER:-openmrs}
      OMRS_CONFIG_CONNECTION_PASSWORD: ${OPENMRS_DB_PASSWORD:-openmrs}
      OMRS_OCL_TOKEN: ${OMRS_OCL_TOKEN:-openmrs}
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/openmrs" ]
      timeout: 5s
    volumes:
      - openmrs-data:/openmrs/data
      #- ./keycloak/oauth2.properties:/openmrs/data/oauth2.properties:ro
    networks:
      - backend-network
      - database-network
      - auth-network
      - dicom-network

      # MariaDB master
  db:
    container_name: peruHCE-db-master
    image: mariadb:10.11.7
    restart: "unless-stopped"
    command: "mysqld --character-set-server=utf8mb4 --collation-server=utf8mb4_bin --server-id=1 --log-bin=mysql-bin --binlog-format=ROW --sync-binlog=1 --log-slave-updates=1"
    #command: "mysqld --character-set-server=utf8mb3 --collation-server=utf8mb3_general_ci"
    healthcheck:
      test: "mysql --user=${OMRS_DB_USER:-openmrs} --password=${OMRS_DB_PASSWORD:-openmrs} --execute \"SHOW DATABASES;\""
      interval: 3s
      timeout: 1s
      retries: 5
    #env_file: "enviromentVariables.env"
    environment:
      MYSQL_DATABASE: openmrs
      MYSQL_USER: ${OMRS_DB_USER:-openmrs}
      MYSQL_PASSWORD: ${OMRS_DB_PASSWORD:-openmrs}
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:-openmrs}
      OMRS_DB_REPL_USER: ${OMRS_DB_REPL_USER:-openmrs_repl}
      OMRS_DB_REPL_PASSWORD: ${OMRS_DB_REPL_PASSWORD:-openmrs_repl}
      OMRS_DB_BACKUP_USER: ${OMRS_DB_BACKUP_USER:-openmrs_b}
      OMRS_DB_BACKUP_PASSWORD: ${OMRS_DB_BACKUP_PASSWORD:-openmrs_b}

    ports:
      - "3307:3306" # 3307 for host
    volumes:
      - ./scripts/database/init-master.sh:/docker-entrypoint-initdb.d/init-master.sh
      - db-data:/var/lib/mysql
      - db-backup:/backup
    networks:
      - database-network

  # MariaDB Replic
  db-replic:
    container_name: peruHCE-db-replic
    image: mariadb:10.11.7
    restart: "unless-stopped"
    command: "mysqld --character-set-server=utf8mb4 --collation-server=utf8mb4_bin --server-id=2 --log-bin=mysql-bin --binlog-format=ROW --sync-binlog=1 --log-slave-updates=1 --read-only=1"
    healthcheck:
      test: "mysql --user=${OMRS_DB_USER:-openmrs} --password=${OMRS_DB_PASSWORD:-openmrs} --execute \"SHOW DATABASES;\""
      interval: 3s
      timeout: 1s
      retries: 5
    #env_file: "enviromentVariables.env"
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:-openmrs}
      OMRS_DB_REPL_USER: ${OMRS_DB_REPL_USER:-openmrs_repl}
      OMRS_DB_REPL_PASSWORD: ${OMRS_DB_REPL_PASSWORD:-openmrs_repl}
      OMRS_DB_BACKUP_USER: ${OMRS_DB_BACKUP_USER:-openmrs_b}
      OMRS_DB_BACKUP_PASSWORD: ${OMRS_DB_BACKUP_PASSWORD:-openmrs_b}
    depends_on:
      - db
    ports:
      - "3308:3306" # 3308 for host
    volumes:
      - ./scripts/database/init-slave.sh:/docker-entrypoint-initdb.d/init-slave.sh
      - db-data-r:/var/lib/mysql
      - db-backup-r:/backup
    networks:
      - database-network

  # DNS Server - Pi-hole for On-Premise Hospital Network
  dns:
    container_name: peruHCE-dns
    image: pihole/pihole:2025.03.0
    restart: "unless-stopped"
    ports:
      # DNS Ports - Critical for hospital network
      - "53:53/tcp"
      - "53:53/udp"
      # Web UI Port
      - "81:80/tcp"
      # HTTPS Port
      - "444:443/tcp"
    env_file:
      - ./.env
    environment:
      # Timezone
      TZ: 'America/Lima'

      # Web UI Password
      FTLCONF_webserver_api_password: ${DNS_PASSWORD:-openmrs}
      FTLCONF_dns_listeningMode: 'all'

      # Upstream DNS servers (when internet available)
      PIHOLE_DNS_: '8.8.8.8;8.8.4.4;1.1.1.1'

      # Security
      DNSSEC: 'true'

      # Local network configuration
      DNS_BOGUS_PRIV: 'false'     # Allow private IP queries
      DNS_FQDN_REQUIRED: 'false'   # Flexible with local names

      # Reverse DNS for local network
      REV_SERVER: 'true'
      REV_SERVER_DOMAIN: 'hospital.local'
      REV_SERVER_TARGET: ${HOSPITAL_GATEWAY:-192.168.1.1}  # Hospital gateway
      REV_SERVER_CIDR: ${HOSPITAL_NETWORK:-192.168.1.0/24} # Hospital network

      # Cache optimization for limited internet
      CACHE_SIZE: '10000'         # Large cache (default 10000)

      # Query logging for auditing
      QUERY_LOGGING: 'true'

    volumes:
      # Persistent data
      - pihole-data:/etc/pihole
      - pihole-dnsmaq:/etc/dnsmasq.d

      # Custom DNS mappings for hospital
      - ./dns/custom.list:/etc/pihole/custom.list:ro

    networks:
      - management-network
      - frontend-network  # Allow resolution for public services

  fua-generator:
    container_name: peruHCE-fua-generator
    image: marcelius733/fua-generator:dev-0.1.4
    ports:
      - "3333:3000"
    #env_file: "enviromentVariables.env"
    environment:
      DB_USER: ${PERUHCE_FUA_GEN_DB_USER:-fuagenerator}
      DB_PASSWORD: ${PERUHCE_FUA_GEN_DB_PASSWORD:-fuagenerator}
      DB_NAME: ${PERUHCE_FUA_GEN_DB:-fuagenerator}
      DB_HOST: 'fua-generator-db'
      DB_PORT: '5432'
      TOKEN: ${PERUHCE_FUA_GEN_TOKEN:-fuagenerator}
    depends_on:
      - fua-generator-db
    networks:
      - services-network
      - backend-network

  fua-generator-db:
    container_name: peruHCE-fua-generator-db
    image: postgres:15
    ports:
      - "5433:5432"
    #env_file: "enviromentVariables.env"
    environment:
      POSTGRES_USER: ${PERUHCE_FUA_GEN_DB_USER:-fuagenerator}
      POSTGRES_PASSWORD: ${PERUHCE_FUA_GEN_DB_PASSWORD:-fuagenerator}
      POSTGRES_DB: ${PERUHCE_FUA_GEN_DB:-fuagenerator}
    volumes:
      - db-fua-generator:/var/lib/postgresql/data
    networks:
      - services-network

  # Orthanc DICOM Server
  orthanc:
    container_name: peruHCE-orthanc
    image: osimis/orthanc:24.12.4
    restart: "unless-stopped"
    ports:
      - "8042:8042"  # HTTP port
      - "4242:4242"  # DICOM port
    environment:
      ORTHANC_JSON: |
        {
          "Name": "PeruHCE Orthanc",
          "RemoteAccessAllowed": true,
          "AuthenticationEnabled": false,
          "RegisteredUsers": {},
          "DicomAet": "ORTHANC",
          "DicomPort": 4242,
          "DicomWeb": {
            "Enable": true,
            "Root": "/dicom-web/",
            "EnableWado": true,
            "WadoRoot": "/wado",
            "Ssl": false
          },
          "StableAge": 60,
          "StrictAetComparison": false,
          "HttpTimeout": 60,
          "ConcurrentJobs": 2
        }
    volumes:
      - orthanc-data:/var/lib/orthanc/db
    networks:
      - dicom-network

  # OHIF Viewer
  ohif:
    container_name: peruHCE-ohif
    image: ohif/app:v3.9.3
    restart: "unless-stopped"
    ports:
      - "3000:80"
    environment:
      APP_CONFIG: |
        {
          "routerBasename": "/",
          "servers": {
            "dicomWeb": [
              {
                "name": "PeruHCE Orthanc",
                "wadoUriRoot": "http://orthanc:8042/wado",
                "qidoRoot": "http://orthanc:8042/dicom-web",
                "wadoRoot": "http://orthanc:8042/dicom-web",
                "qidoSupportsIncludeField": true,
                "imageRendering": "wadors",
                "thumbnailRendering": "wadors",
                "requestOptions": {
                  "requestFromBrowser": true
                }
              }
            ]
          },
          "defaultDataSourceName": "dicomWeb"
        }
    depends_on:
      - orthanc
    networks:
      - dicom-network
      - frontend-network

  # Prometheus - Metrics collection
  prometheus:
    container_name: peruHCE-prometheus
    image: prom/prometheus:v3.1.0
    restart: "unless-stopped"
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/alerts:/etc/prometheus/alerts:ro
      - prometheus-data:/prometheus
    networks:
      - monitoring-network
      - backend-network
      - database-network
      - auth-network
      - dicom-network
      - services-network

  # Loki - Log aggregation
  loki:
    container_name: peruHCE-loki
    image: grafana/loki:3.3.2
    restart: "unless-stopped"
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/loki-config.yml
    volumes:
      - ./monitoring/loki/loki-config.yml:/etc/loki/loki-config.yml:ro
      - loki-data:/loki
    networks:
      - monitoring-network

  # Promtail - Log shipper for Loki
  promtail:
    container_name: peruHCE-promtail
    image: grafana/promtail:3.3.2
    restart: "unless-stopped"
    volumes:
      - ./monitoring/promtail/promtail-config.yml:/etc/promtail/promtail-config.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
    command: -config.file=/etc/promtail/promtail-config.yml
    depends_on:
      - loki
    networks:
      - monitoring-network

  # Grafana - Visualization and dashboards
  grafana:
    container_name: peruHCE-grafana
    image: grafana/grafana:11.4.0
    restart: "unless-stopped"
    ports:
      - "3001:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin123}
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource,grafana-piechart-panel
      GF_SERVER_ROOT_URL: http://localhost:3001
      GF_USERS_ALLOW_SIGN_UP: "false"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
      - loki
    networks:
      - monitoring-network
      - management-network

volumes:
  keycloak-data: ~
  openmrs-frontend: ~
  openmrs-data: ~
  db-data: ~
  db-backup: ~
  db-data-r: ~
  db-backup-r: ~
  db-fua-generator: ~
  gateway-data: ~
  portainer-data: ~
  pihole-data: ~
  pihole-dnsmaq: ~
  orthanc-data: ~
  prometheus-data: ~
  loki-data: ~
  grafana-data: ~

networks:
  # Red pública - Servicios expuestos al exterior
  frontend-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24

  # Red de aplicación - APIs y lógica de negocio
  backend-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/24

  # Red de datos - Bases de datos (INTERNA - sin acceso externo directo)
  database-network:
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: 172.22.0.0/24

  # Red de autenticación - Keycloak y SSO
  auth-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.23.0.0/24

  # Red médica - DICOM/Orthanc/OHIF
  dicom-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.24.0.0/24

  # Red de monitoreo - Prometheus, Loki, Grafana
  monitoring-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/24

  # Red de gestión - Portainer, herramientas admin
  management-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.26.0.0/24

  # Red de servicios auxiliares - FUA Generator, etc
  services-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.27.0.0/24
